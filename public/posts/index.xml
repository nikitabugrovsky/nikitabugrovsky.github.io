<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on nikitabugrovsky</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on nikitabugrovsky</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <copyright>© Nikita Bugrovsky.

Opinions are mine &amp; may not reflect the position of a company I work for.</copyright>
    <lastBuildDate>Wed, 06 Aug 2025 21:05:03 +0300</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What Can We Learn About LLMs From a Simple Weather Bot</title>
      <link>http://localhost:1313/posts/what-can-we-learn-about-llms-from-a-simple-weather-bot/</link>
      <pubDate>Wed, 06 Aug 2025 21:05:03 +0300</pubDate>
      <guid>http://localhost:1313/posts/what-can-we-learn-about-llms-from-a-simple-weather-bot/</guid>
      <description>&lt;p&gt;When we build applications with Large Language Models (LLMs), one of the most powerful capabilities we can unlock is function calling. This post explores how two different models, Google&amp;rsquo;s Gemini and the open model Gemma, approach this task. While both rely on a shared philosophy—the Natural Language Interface (NLI)—their implementations reveal two distinct engineering experiences.&lt;/p&gt;&#xA;&lt;p&gt;The idea for this weather bot was born from the official &lt;a href=&#34;https://ai.google.dev/gemini-api/docs/function-calling?example=weather&#34;&gt;Google Gemini documentation on function calling&lt;/a&gt;, which makes it clear that while the LLM can &lt;em&gt;suggest&lt;/em&gt; calling a function, it is our responsibility to implement &lt;strong&gt;and&lt;/strong&gt; call that function.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
